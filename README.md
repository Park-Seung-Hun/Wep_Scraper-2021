# webscraper-2021
 
### ğŸ“– webscraper-2021
> íŒŒì´ì¬ì„ ì´ìš©í•˜ì—¬ êµ¬ì§ ì‚¬ì´íŠ¸ì˜ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í”„ë¡œê·¸ë¨ì„ êµ¬í˜„í•´ë³´ì•˜ë‹¤. <br>


### âœ… ì‚¬ìš© Skills
  1. HTML
  2. Python
  3. Flask


### ğŸ“• ì£¼ìš” ê¸°ëŠ¥
1. ìŠ¤í¬ë˜í•‘
<img src="https://github.com/Park-Seung-Hun/webScraper-2021/blob/main/resultImage/%EC%9B%B9%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91.gif" width="80%">
2. ì¶”ì¶œ 
<img src="https://github.com/Park-Seung-Hun/webScraper-2021/blob/main/resultImage/%EC%B6%94%EC%B6%9C.gif" width="80%">
<img src="https://github.com/Park-Seung-Hun/webScraper-2021/blob/main/resultImage/2021-03-17.png" width="80%">

[csv íŒŒì¼ ë³´ëŸ¬ê°€ê¸°](https://github.com/Park-Seung-Hun/webScraper-2021/blob/main/resultImage/flask%20(1).csv)
#### `main.py`
> ì›¹ ìŠ¤í¬ë˜í¼ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ í†µì œí•˜ê¸° ìœ„í•´ ë§Œë“  íŒŒì¼ë¡œ 3ê°€ì§€ ì£¼ìš” í•¨ìˆ˜ë¥¼ ë‹´ë‹¹í•œë‹¤. [main ì½”ë“œ ë³´ëŸ¬ê°€ê¸°](https://github.com/Park-Seung-Hun/webScraper-2021/blob/main/main.py)<br>

 1. app.route("/") : ê¸°ë³¸ í™”ë©´(main.htmlì„ ì›¹ ìƒì— ë„ìš´ë‹¤.)
```python
@app.route("/") # rootì— ì ‘ì†í•˜ë©´ homeì´ë¼ëŠ” í•¨ìˆ˜ ì‹¤í–‰
def home():
    return render_template("main.html") # main.htmlì„ ë Œë”ë§
```
 2. app.route("/report") : ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì—¬ ì›¹ ìŠ¤í¬ë˜í•‘.
 ```python
 @app.route("/report")
 def report():
    word = request.args.get("word") # í•´ë‹¹ í˜ì´ì§€ì˜ word ê°’ì„ ë°›ì•„ì˜¨ë‹¤.
    
    if word: 
        word = word.lower() # ì…ë ¥ë˜ëŠ” ê²€ìƒ‰ì–´ë¥¼ ëª¨ë‘ ì†Œë¬¸ìë¡œ ë°”ê¾¼ë‹¤.
        existingJobs = db.get(word) # ì„ì‹œ databaseë¥¼ ì„¤ì •
        if existingJobs: # ì´ë¯¸ ê²€ìƒ‰í–ˆë˜ ê²€ìƒ‰ì–´ ì¼ë•Œ
            jobs = existingJobs
        else: # ê²€ìƒ‰í•œ ì ì´ ì—†ì„ ë•Œ
            jobs = get_jobs(word) # scrapperì˜ get_jobs í•¨ìˆ˜ ì‹¤í–‰
            db[word] = jobs
    else: # rootë¡œ ëŒì•„ê°„ë‹¤.
        return redirect("/")
        
    return render_template("report.html", 
        searchingBy=word,
        resultNum=len(jobs),
        jobs=jobs) # report.html ì„ ë Œë”ë§ í•˜ëŠ”ë°, ë°›ì€ ë³€ìˆ˜ë“¤ì„ ê°™ì´ ë³´ë‚¸ë‹¤.
 ```

 3. app.route("/export") : ìŠ¤í¬ë˜í•‘í•œ ì •ë³´ë¥¼ excelë¡œ ì¶”ì¶œ.
```python
@app.route("/export")
def export():
    try:
        word = request.args.get('word')
        
        if not word:
            raise Exception()

        word = word.lower()
        existingJobs = db.get(word)

        if not existingJobs:
            raise Exception()

        save_to_file(existingJobs,word)
        file_name=f"{word}.csv"
        return send_file(file_name,
            mimetype='text/csv',
            as_attachment=True,
            attachment_filename = file_name)
    except:
        return redirect("/")
```
#### `scrapper.py` [scrapper ì½”ë“œ ë³´ëŸ¬ê°€ê¸°](https://github.com/Park-Seung-Hun/webScraper-2021/blob/main/scrapper.py)
```python
# 1. í˜ì´ì§€ì˜ ìµœëŒ€ ìˆ˜ë¥¼ ë°›ì•„ì˜¨ë‹¤.
def get_last_pages(url):
    result = requests.get(url)
    soup = BeautifulSoup(result.text,"html.parser")
    pages = soup.find("div",{"class":"s-pagination"}).find_all("a")
    last_page = pages[-2].get_text(strip=True)
    return int(last_page)

# 3. ì „ë¶€ ë¶ˆëŸ¬ì˜¨ ì¼ìë¦¬ ì •ë³´ë“¤ì„ ê°ê°ì˜ ì„¸ë¶„í™”ëœ ì •ë³´ë¡œ ë‚˜ëˆˆë‹¤.(ì œëª©,íšŒì‚¬,ì§€ì—­,ì§€ì› ë§í¬)
def extract_job(html):
    title = html.find("h2",{"class":"mb4"}).find("a")["title"] # ì§ì¢…
    company, location = html.find("h3",{"class":"fc-black-700"}).find_all("span",recursive=False) # recursive = FalseëŠ” ì „ë¶€ ê°€ì ¸ì˜¤ëŠ”ê±¸ ë°©ì§€(ì²«ë‹¨ê³„ë§Œ)
    company = company.get_text(strip=True)
    location = location.get_text(strip=True)
    job_id= html["data-jobid"]

    return {'title': title,'company': company,'location': location, 'apply_link': f"https://stackoverflow.com/jobs/{job_id}"}

# 2. í˜ì´ì§€ë§ˆë‹¤ ì¼ìë¦¬ ì •ë³´ë“¤ì„ ì „ë¶€ ë¶ˆëŸ¬ì˜¨ë‹¤. 
def extract_jobs(last_page,url):
    jobs = []
    for page in range(last_page):
        print(f"Scrapping SO: page: {page}")
        result = requests.get(f"{url}&pg={page+1}")
        soup = BeautifulSoup(result.text,"html.parser")
        results = soup.find_all("div",{"class":"-job"}) # ì¼ìë¦¬ ì •ë³´ê°€ ë“¤ì–´ìˆìŒ

        for result in results:
            job = extract_job(result)
            jobs.append(job)
    return jobs        

# mainì—ì„œ ì…ë ¥í•œ ê²€ìƒ‰ì–´ë¥¼ í†µí•´ í˜ì´ì§€ ì •ë³´ ìŠ¤í¬ë˜í•‘ í›„ ì •ë³´ë¥¼ ì„¸ë¶„í™”í•´ ë‹¤ì‹œ mainìœ¼ë¡œ return.
def get_jobs(word):
    URL = f"https://stackoverflow.com/jobs?q={word}"
    last_page = get_last_pages(URL)
    jobs = extract_jobs(last_page,URL)
    return jobs
```
#### `exporter.py` [exporter ì½”ë“œ ë³´ëŸ¬ê°€ê¸°](https://github.com/Park-Seung-Hun/webScraper-2021/blob/main/exporter.py)
```python
# mainì—ì„œ ìŠ¤í¬ë˜í•‘í•˜ê³  ì„¸ë¶„í™”í•œ ì§ì—… ì •ë³´ì™€ íŒŒì¼ëª…ì„ ë°›ì•„ì™€ .csvíŒŒì¼ë¡œ ì €ì¥í•œë‹¤.
def save_to_file(jobs,word):
    file = open(f"{word}.csv", encoding='UTF-8',mode="w", newline='') # mode w ì“°ê¸°, r ì½ê¸°
    writer = csv.writer(file)
    writer.writerow(["title", "company", "location", "link"])
    
    for job in jobs:
        writer.writerow(list(job.values()))
    return 
```

#### `report.html`
```html
<!--ì •ë³´ë¥¼ ë°›ì•„ì™€ html íŒŒì¼ì— ë‚˜íƒ€ë‚¸ë‹¤.-->
 <body>
    <h1>Search Result</h1>
    <h3>Found {{resultNum}} results for: {{searchingBy}}</h3>
    <a href="/export?word={{searchingBy}}">Export to CSV</a>
    <section>
      <h4>Title</h4>
      <h4>Company</h4>
      <h4>Location</h4>
      <h4>Link</h4>

      {% for job in jobs %}
      <span>{{job.title}}</span>
      <span>{{job.company}}</span>
      <span>{{job.location}}</span>
      <a href="{{job.apply_link}}" target="blank">Apply</a>
      {% endfor %}
    </section>
  </body>
```

### ğŸ“˜ ì¶”ê°€í•  ê¸°ëŠ¥


### ğŸ“™ ì¶œì²˜
[ë…¸ë§ˆë“œ ì½”ë”](https://nomadcoders.co/)<br>



